---
layout: documentation
title: "Framework for Monitoring and Evaluating Inclusive Technologies in Social Change Projects"
date: 2016-05-03 14:25:00
author: Laura Walker McDonald
categories: [documentation]
tags: [Training, Course, "Disaster Response"]
menus: [downloads.html, section_links.html, read_stories.html]

sections:
    - { link: "section1",
        text: "How SIMLab understands monitoring and evaluation"}
    - { link: "section2",
        text: "About this Framework"}
    - { link: "section3",
        text: "About SIMLab"}
    - { link: "section4",
        text: "How SIMLab works"}
    - { link: "section5",
        text: "Why create a Framework for M&E of Inclusive Technologies in social change projects?"}
    - { link: "section6",
        text: "SIMLab’s Evaluation Criteria"}
    - { link: "section10",
        text: "Text Elements",
        description: "Styling of common text elements" }
---

##  {% include html/sectionlink.html link="section1" %}How SIMLab understands monitoring and evaluation

A monitoring and evaluation (M&E) process is put in place for 3 main purposes:

- As a management tool to drive change
- As an accountability tool
- To provide lessons and learning

M&E may also be used to inform future funding decisions; judge the performance of contractors; or to gather evidence to establish whether a particular approach is useful. In SIMLab’s case, we are also interested in examining how a particular inclusive technology, or inclusive technology overall, contributes to wider programmatic goals. Our M&E findings should thoroughly test, and may serve to prove or disprove the validity of certain approaches to using inclusive technology.

### What do we mean by ‘inclusive technologies’?

{:.highlight .magenta-border}
We define inclusive technologies broadly: those that have broad reach, relatively low costs, are easy to use, rely on existing infrastructure, and use common data formats. Examples of inclusive technologies include SMS, radio, voice telephony, even blackboards and megaphones. They can be knit together to extend accessible systems and services to hard-to-reach populations.

Monitoring and evaluation are two different phases of one cyclical process which influences all phases of a program and, ideally, feeds into future program design.

*Monitoring* refers to an on-going, periodic process of tracking implementation with the primary purpose of informing day-to-day project management decisions and tracking how an initiative is progressing. In some programs, monitoring includes “real-time” data and feedback from program participants that can inform immediate decisions.

*Evaluation* is more of a discrete activity, which refers to the systematic and objective assessment of an ongoing or completed project or program which looks at its design, implementation and results. Evaluations may also aim to determine the worth of an activity, intervention or program.

A third concept, *review*, is that of an assessment of the performance of an intervention periodically or on an ad-hoc basis. Evaluations tend to be more comprehensive, and reviews often focus on operational aspects rather than wider impact. (OECD 2002)

For us, *learning* is also a critical element, in which we ensure that the insights we have gained from our M&E are shared within our SIMLab team, and wherever possible, with others, in easily-digestible formats. Learnings should also inform best-practice guidance like this Framework, and contribute to our understanding of what it is to do good inclusive technology work.


## {% include html/sectionlink.html link="section2" %}About this Framework

This M&E Framework aims to guide SIMLab staff in measuring our work and determining, to the degree possible, the contribution of inclusive technology to the outcomes and impact of our implementation projects.

What follows is intended as a minimum M&E standard for SIMLab staff to follow at each phase of the program lifecycle (planning, implementation and monitoring, evaluation, and dissemination of learning). However, the Framework includes guidance and tools that can be useful for projects at any stage. It should supplement and refer to existing M&E best practice resources, rather than seeking to rewrite or replace them.

The Framework can be applied across SIMLab’s continuum of program and partnership modalities, whether SIMLab is implementing or conducting M&E directly, partnering with a larger program, working with a community-based organization, or some other set-up.

As with all our learning and best practice resources, the Framework is shared publicly

1. So that our partners can refer to it, and potentially adapt elements of it in their own work;
2. So that others can comment on and improve the Framework, and
3. As a contribution to the thinking of the wider sector about challenges and approaches to teasing out the contribution that inclusive technologies make to social change work

This project is a work in progress that will be publicly available under an open license, and regularly updated and improved with support from the ICT4D, aid and development communities in the hopes that it might serve as a resource for others who are working with inclusive technology. We gratefully recognize that this work was made possible by support from the UK Department of International Development and the Hewlett Foundation.

### What this Framework Doesn’t Cover

This framework is not aimed at providing guidance on program design and planning. However, a good understanding of the M&E process and areas that would be assessed in an evaluation are useful for informing program design and it’s important to build learning from monitoring and learning into program design and implementation.

Additionally, we do not delve into the range of ways that technology can support M&E itself; e.g., using technological devices to collect, analyze and visualize data for M&E of programs; whether the projects themselves use technology or not. This is covered in depth in other resources highlighted in the bibliography (e.g. Bamberger & Raftree, 2014).

At present, this Framework focuses on M&E at the project level, and does not seek to support evaluation of the overall impact of technology on a field or a broad geographic area.

## {% include html/sectionlink.html link="section3" %} About SIMLab

Social Impact Lab (SIMLab) helps to build accessible, responsive and resilient systems using inclusive technologies, helping people and organizations solve both the technological and human obstacles along the way. SIMLab believes that equitable participation of marginalized and ‘last-mile’ populations in public, economic, and social life contributes to a more just world. We believe that increasing systemic adoption and use of inclusive technologies leads to greater access to services for all populations, accountability and responsiveness of institutions, and resilience of societies.

### Our organizational principles

In 2015, SIMLab developed core principles to guide us in our work and behavior, with colleagues and partners, and in our decision-making. Two in particular prioritize learning from our work and sharing our findings.

{:.highlight .magenta}
Commit to learning from our work and operationalizing what we learn; tolerate risk; and acknowledge failures.

*We will invest in processing, documenting, and operationalizing learning from our implementation work, and work towards doing so with our best practice and advocacy work. We proactively share learning with others as case studies, published evaluations, blog posts, and tools and guides, all under open, attributable licenses. We publicly acknowledge our successes and failures at all levels of the organization and in all our areas of work.*

*We will invest in creative communications strategies to make this meaningful and impactful, such as podcasts, webinars, events, long and short format written papers, and short blog posts. We will share resources in accessible and inclusive formats, using simple, clear language. We will translate resources where we can.*

*As part of the learning process, we will conduct rigorous monitoring and evaluation on our projects, implementing changes as needed. Monitoring will be used to make adjustments to programs as they are happening, and evaluations will be conducted at the end to ensure we are holding ourselves accountable. It is not enough to just present the data and findings. SIMLab will include a recommendations section in each evaluation that takes the findings and presents several opportunities for institutional learning and change. Making and implementing recommendations ensures that we and others are able to usefully apply our M&E efforts to our future work.*

*We recognize that operationalizing this principle means being willing to ask our donors, supporters and partners to structure partnerships that allow agility in our ideas and projects, challenging the power structures inherent in much of social change work. We recognize that this may narrow our ability to work with some actors.*

{:.highlight .magenta}
Encourage ecosystems of collaboration and openness; and as far as possible use existing tools, platforms and resources rather than creating our own

*Coming from a history of software development and close relations with platform providers, we are strong advocates for open design elements, like APIs and building on common platforms, that lead to interoperable systems and an ecosystem of mutually supportive products. Our commitment to sharing learning and support of open licensing is linked to this.*

*...We seek always to be collaborative, rather than competitive, and to break down silos between sectors and specialisms.*

{:.highlight .magenta}
Encourage ecosystems of collaboration and openness; and as far as possible use existing tools, platforms and resources rather than creating our own

## {% include html/sectionlink.html link="section4" %}How SIMLab works

SIMLab projects follow a range of modalities, loosely broken down below, with guidance on what might be required in each case. This is advisory only - this is always a judgement call and should be signed off by the Head of Programs/CEO. Where no one category perfectly fits a particular situation, be guided by those which are closest to true.

__Advisory capacity, or consortium partner without direct responsibility for implementation or aspects of M&E__

Here, SIMLab may be working with a partner with an existing M&E approach. Use this guide to help them supplement their lines of enquiry, and where necessary, their methodology, with inclusive technology-focussed issues. We may want to consider our role in the process, e.g. collecting information directly, or influencing the terms of reference or grant agreement so that M&E on the contribution of inclusive technology is included.

If they do not plan to expend any resources on M&E, consider whether the project is likely to meet the standard set by our principles (see above), and discuss our continued involvement and ongoing strategy with the Head of Programs/CEO.

__Lead implementer, or consortium partner with direct responsibility for aspects of M&E__

Here we would expect to develop an M&E approach for the project, the scale of which should be informed by

- The scale of the proposed project
- Available time and budget for M&E
- Partner capacity to conduct M&E, and mandate to or interest in conducting M&E on technology
- The research or evidence aspirations of the project team
- Partner (and donor) openness to change to the planned approach based on incoming monitoring information
- Partner interest in learning and sharing lessons from implementation
- Availability of existing relevant data and analyses

In some cases, the only, and often most appropriate way to make space for M&E on the technology aspects is to tag a few questions onto an existing program evaluation design.

Even when an external party does not require an evaluation, SIMLab should capture and document learning to improve future efforts. SIMLab should conduct its own internal evaluation process when:

- Our portion of the project funding exceeded $50,000 (between $10,000 and $50,000, an After-Action Review meeting is advisable)
- We are testing a new project approach, technique or way of working, and wish to document learning from it
- The Project Steering Group feels that our implementation has experienced challenges which should be formally documented
- There is no planned evaluation, or the evaluation is not likely to capture learning relevant to SIMLab.

## {% include html/sectionlink.html link="section5" %}Why create a Framework for M&E of Inclusive Technologies in social change projects?

The use of inclusive technologies in development and social change is maturing, and should move beyond pilots and prototypes to longer-term interventions, grounded in existing learning and best practice, with more rigorous evaluations that specifically review the contribution that inclusive technologies make to our work. To date, robust evidence of the contribution that inclusive technologies make has emanated more from research than from project-level M&E.

Pressure is growing to bring this evidence to bear, and to move beyond continually re-committing the same mistakes. We have seen a new effort to consolidate best practice, establish measures of impact for programming that include the contributions of newer kinds of technology components, and focus energy on new boundaries of scale and effectiveness. This can be seen in the [Principles for Digital Development](http://digitalprinciples.org/){:target="_blank"}, as well as sector-specific efforts like DFID’s [Conflict, Crime and Violence Results Initiative](https://www.gov.uk/government/publications/conflict-crime-and-violence-results-initiative-good-practice-guides-on-security-and-justice-issues){:target="_blank"}, which offers a series of papers and guides on good practice in security and justice issues (see Bibliography for more information on these resources).

Accordingly, more emphasis needs to be given to: how well a particular channel, tool, or platform works in a given scenario; how it contributes to development goals in combination with other channels and tools; how the team selected and deployed it; and whether it is a better choice than not using technology or using a different sort of technology.

This doesn’t mean that M&E tools, guidelines, and systems that are used for other scenarios are not relevant to inclusive technologies. Rather, there are additional considerations, and particularly relevant approaches, that we propose might be helpful to consider in constructing an M&E plan.

How well the ideas shared in this guide work will depend on the skills and capacities of those using them. Some users of this toolkit may not be familiar with M&E processes, and for them, we recommend further reading, for example, at the Better Evaluation website (see bibliography for further suggestions). SIMLab staff should discuss training options with their line manager. Other users may be very familiar with M&E, but less familiar with inclusive technologies, in which case we recommend using this guide to improve understanding of the various nuances of monitoring and evaluation technology-enabled programming.

### What’s different about M&E of inclusive technologies in social change projects?

Technology often adds an additional layer of complexity to an already complex project. A huge range of factors are in play, including the technology itself, the content or messaging being passed through the technology, the organization managing the response to communication via the technology, the network, cultural factors, capacity and the skills of an individual who is managing training about the technology. Technology tools may be used in conjunction with one another. Determining the exact contribution of technology to a wider program goal, and the wider, unintended consequences of it, is therefore quite complex.

Technology projects are frequently new operational partnerships: technologists piloting with implementing agencies, perhaps together with research organizations or an involved donor. These actors may have different priorities in terms of things to measure.

Technologist partners, such as those providing the platform or tool, may be involved only in the early stages of a project - disappearing from active involvement after development and rollout are complete. They may be accustomed to very quick cycles of prototyping, testing, and iterating, which may not be well understood or documented by traditional M&E professionals, and which may elapse before M&E can contribute new learning to the design. Technologists also may be most interested in, and may build analytics to measure the effectiveness of the user interface, or usability of a tool, rather than the longer term impact of a wider effort. On the other hand, implementing partners for whom technology is a new operational lens may not have a clear idea of what to look for, how to measure the success of the technology roll-out itself, or how to track and assess the ways that a technology component is (or is not) contributing to wider impact or change.

Additionally, some of the impacts most keenly anticipated by technologists - improvements to efficiency, effectiveness, and ease of communication, for example - are often indirect contributions to a larger social, economic, or political goal. These impacts represent the ‘business-case’ argument for incorporating better technology into any program’s operations, and are only indirect contributions to the development-focused goal which traditional M&E focuses on. Technology may be used for staff management and communications, helplines or incident reporting, stock tracking, and data collection, all of which are more akin to capacity-building than the kinds of innovation that attract awards and headlines. Baseline information of this type is particularly hard to come by from smaller partners, leading to a lack of hard data on changes after implementation, even if anecdotal evidence shows positive results. In addition, it is more difficult to understand technology’s contribution to impact and community-level change under these circumstances.

Finally, some aspects need to be addressed which are not familiar to traditional implementers. SIMLab’s evaluation criteria include sustainable business models, ethical data practices, and security, privacy and protection, in addition to organisational development practice and support for innovation. Data security questions are particularly difficult, as program staff may end up handling private data and personally identifiable information without a clear understanding of ethical concerns and information management practices, much less legal policies or frameworks, to guide them in the security and storage of these data. In addition, there are particular concerns that arise when it comes to privacy and protection, especially of vulnerable populations, once new technologies are incorporated, and complex continuums of risk and behaviors that play off one another when traditional and digital processes or activities are involved.

Accordingly, few resources are allocated to conducting rigorous M&E on the role of inclusive technologies. Smaller organizations tend to have limited capacity for conducting M&E, and larger organizations’ M&E teams often lack experience measuring the role and contribution of inclusive technologies to impact. Because the focus of the evaluation is on the wider impact rather than the role of the technology itself, there may be little motivation to work on teasing out and understanding the contributions of inclusive technologies and systems. There may also be little interest in analyzing the more systems-oriented improvements to efficiency and information management that may occur where the technology and human aspects meet, where they don’t directly impact program delivery. Platform developers, such as Ushahidi or FrontlineSMS, who might be more motivated, normally rely on partners to conduct M&E, because their platforms are part of wider programs.

In SIMLab’s experience, this leads to a shortage of concrete evidence of the impact of technology in development and aid programs, and in particular comparative data on different platforms, approaches and strategies. As the field matures, it is appropriate to try to build this evidence base where we can.

## {% include html/sectionlink.html link="section6" %} SIMLab’s Evaluation Criteria

In this section we outline SIMLab’s criteria for evaluating inclusive technology. They should be read as supplemental to the OECD-DAC criteria, rather than replacing them.

- __Relevance__ - The extent to which the technology choice is appropriately suited to the priorities, capacities and context of the target group or organization.
- __Effectiveness__ - A measure of the extent to which an information and communication channel, technology tool, technology platform, or a combination of these attains its objectives.
- __Efficiency__ - Efficiency measures the outputs -- qualitative and quantitative -- in relation to the inputs. It is an economic term which signifies that the project or program uses the least costly technology possible in order to achieve the desired results. This generally requires comparing alternative approaches (technological or non-technological) to achieving the same outputs, to see whether the most efficient tools, platforms, channels and processes have been adopted.
- __Impact__ - The positive and negative changes produced by the introduction or change in a technology tool or platform on the overall development intervention, directly or indirectly, intended or unintended. This involves the main impacts and effects resulting from the technology tool or platform on the local social, economic, environmental and other development indicators. The examination should be concerned with both intended and unintended results and must also include the positive and negative impact of external factors, such as changes in terms of trade and financial conditions and digital information and communication ecosystems.
- __Sustainability__ Sustainability is concerned with measuring whether the benefits of a technology tool or platform are likely to continue after donor funding has been withdrawn. Projects need to be environmentally as well as financially sustainable.
- __Coherence__ Coherence is related to the broader policy context (development, market, communication networks, data standards and interoperability mandates, national and international law) within which a technology was developed and implemented.

These criteria were developed by adapting a set of widely used M&E principles (originally created by the OECD Development Assistance Committee - OECD-DAC) to our context and building on the work that the Active Learning Network for Accountability and Performance (ALNAP) did in 2006 to adapt these criteria to complex humanitarian settings (See box).  

Below we provide an in-depth explanation of SIMLab’s criteria for M&E of inclusive technology programming by showing the original OECD-DAC criteria (in italics), the ALNAP adaptation (in italics and where relevant), and key questions that SIMLab believes should be asked in order to monitor and evaluate inclusive technology programming.

{::options parse_block_html="true"/}
{:.highlight .magenta-border}
<section>
__The DAC Principles for Evaluation of Development Assistance (the OECD-DAC Criteria__

In their 1991 [DAC Principles for Evaluation of Development Assistance](http://www.oecd.org/dac/evaluation/50584880.pdf), the OECD-DAC laid out five principles of evaluation to guide DAC member states. The Principles are further defined in the [Glossary of Key Terms in Evaluation and Results Based Management](http://www.oecd.org/dac/evaluation/2754804.pdf).

These principles were subsequently developed into five specific criteria which are today widely used in development evaluation: (i) relevance, (ii) efficiency, (iii) effectiveness, (iv) impact, and (v) sustainability.

ALNAP later adapted and expanded the criteria specifically for use in evaluating complex emergencies: (i) relevance, (ii) connectedness, (iii) coherence, (iv) coverage, (v) efficiency, (vi) effectiveness, and (vii) impact. (ALNAP, 2006)

The criteria are meant to be used together in a complementary fashion. Better Evaluation [notes](http://betterevaluation.org/evaluation-options/dac_criteria) the following supplementary advice from ALNAP (2006):

- Criteria often overlap, and the same data can be employed for different criteria.

- ALNAP identifies eight cross-cutting themes which evaluators should always carefully consider when employing the DAC criteria: local context; human resources; protection; participation of primary stakeholders; coping strategies and resilience; gender equality; HIV/AIDS; and the environment. While an evaluation need not include every theme, a rational should be considered for excluding any.

- While widely used, the DAC evaluation criteria are too often employed mechanistically. They are a valuable guide for framing questions and designing evaluation, but reliance on them should not prohibit more creative processes for evaluation.

- Feedback has shown that many evaluators employ the DAC criteria to ask questions about results rather than processes. There is, however, much room for the five sets of criteria questions above to prompt consideration not only of ‘what’, but also of ‘why’ – for example, not only “what real difference was made to the beneficiaries as a result of the activity?”, but also “why was that difference made or not made?”

</section>


### Criterion 1: Relevance

__DAC Definition of Relevance:__ The extent to which the aid activity is suited to the priorities and policies of the target group, recipient and donor.

__DAC Guidance on Relevance:__
In evaluating the relevance of a programme or a project, it is useful to consider the following questions:
- To what extent are the objectives of the programme still valid?
- Are the activities and outputs of the programme consistent with the overall goal and the attainment of its objectives?
- Are the activities and outputs of the programme consistent with the intended impacts and effects?

__SIMLab Definition of Relevance__ - The extent to which the technology choice is appropriately suited to the priorities, capacities and context of the target group or organization.

__SIMLab Guidance on Relevance__

In addition to the above DAC orientation on relevance, and drawing on ALNAP’s suggestions under this principle, evaluators should consider:

- To what extent was an analysis of context and an adequate needs assessment conducted? Did the implementor have a good grasp of the context and the communications and information needs and habits of the target population? (cf. also the [SIMLab Context Analysis Framework](https://docs.google.com/document/d/1-RvVky0ubjH1qxP201AvNeCIeTJHsyZ3qGVIK-iUDYM/edit#heading=h.bybgc5qih718)).

- To what extent was there sufficient institutional capacity, staffing capacity, local knowledge and experience in the country or region to implement a relevant and appropriate project? This may include organizational readiness for innovation, capacity to manage technology and infrastructure capacity, among other factors.

- To what extent was the choice of the technology tool context-appropriate and informed by user needs and habits, device ownership, network coverage, literacy and education levels, and other context-specific aspects? How well was the tool designed adequately for the skill level of the intended users?

- To what extent were the target population and other key stakeholders, including staff and management of the implementing organization(s), involved in the design of the communications mechanism, tool or platform? To what extent were they involved in reviewing prototypes and suggesting adjustments?

- To what extent does the implementing organization have the necessary technological and operational capacity to take on the management of technology platforms, manage incoming information, and maintain interactive communications with communities? How does the organizational culture allow for risk tolerance and openness to innovation?

- To what extent does the initiative take into consideration the target population’s existing portfolio of digital communication, tools and platforms? How was content localized for the target groups?

<br/>

{:.light}
<hr/>

### Criterion 2: Effectiveness

__DAC Definition of Effectiveness:__ A measure of the extent to which an aid activity attains its objectives.

__DAC Guidance on Effectiveness:__ In evaluating the effectiveness of a programme or a project, it is useful to consider the following questions:

- To what extent were the objectives achieved / likely to be achieved?

- What were the major factors influencing the achievement or non-achievement of the objectives?

__SIMLab Definition of Effectiveness:__ A measure of the extent to which an information and communication channel, technology tool, technology platform, or a combination of these attains its objectives.

__SIMLab Guidance on Effectiveness:__

In a technology-enabled effort, there may be one tool or platform, or a set of tools and platforms may be designed to work together as a suite. Additionally, the selection of a particular communication channel (SMS, voice, etc) matters in terms of cost and effectiveness.

Note that this criterion should be examined at outcome level, not output level, and should examine how the objectives were formulated, by whom (did primary stakeholders participate?) and why.

With technology, plans often do not long survive contact with reality and adjustments and snags are predictable occurrences. What matters is that feedback and failures were acknowledged, that there were systems and communications channels to deal with them and incorporate learning and required changes, and that the technology challenges did not throw off or undermine the effectiveness of the wider project.

The following questions can serve as a guide for evaluating the Effectiveness criterion.

- To what extent did the selected communications channel harmonize with the information and communication habits and needs of the target population? To what extent did the technology tool(s) or platforms or combination of them meet the information and communication needs of the overall project? How comfortable were implementing partners with the tool/platform? Did users of the tool have access to high-quality support? If multiple systems or communications channels were used, how well did they work together?

- How did the technology tool or platform perform? Is it largely free of bugs and errors? Is it available in the necessary languages or easily translated?

- If a digital process or channel replaced a non-digital one or an existing digital process was enhanced by or replaced with a new one, how did the new digital channel compare to the previous way of doing things? What were the differences in terms of meeting the specific objectives for which the new tool was introduced?



## {% include html/sectionlink.html link="section10" %}Text styling elements

##### Headings

# H1

## H2

### H3

#### H4

##### H5

###### H6

{:.underlined}
### Underlined heading

<br/>

{:.underlined .bold}
### Bold underlined heading

<br/>

{:.underlined .bold .dark}
### Bold and dark underlined heading

<br/>

{:.underlined .bold .magenta}
### Bold magenta underlined heading

<br/><br/><br/>

##### Bold Text

__Lorem ipsum dolor sit amet, assum similique delicatissimi usu no. Fierent laboramus sit ut, cetero eloquentiam eos cu. Vim falli graeco an, usu at quot graeco, sit in accumsan probatus delicata. Debet vitae iracundia per cu, no utinam percipit lobortis usu. No gloriatur persequeris suscipiantur his, te nec modo admodum vulputate, ne pri accumsan placerat detraxit. An ius solum iudico aliquid, vim no consul platonem, amet omittantur vim eu.__

<br/><br/><br/>

##### Blockquote

> Lorem ipsum dolor sit amet, assum similique delicatissimi usu no. Fierent laboramus sit ut, cetero eloquentiam eos cu. Vim falli graeco an, usu at quot graeco, sit in accumsan probatus delicata. Debet vitae iracundia per cu, no utinam percipit lobortis usu. No gloriatur persequeris suscipiantur his, te nec modo admodum vulputate, ne pri accumsan placerat detraxit. An ius solum iudico aliquid, vim no consul platonem, amet omittantur vim eu.

<br/><br/><br/>

##### Highlighted text versions

{:.highlight .magenta}
Lorem ipsum dolor sit amet, assum similique delicatissimi usu no. Fierent laboramus sit ut, cetero eloquentiam eos cu. Vim falli graeco an, usu at quot graeco, sit in accumsan probatus delicata. Debet vitae iracundia per cu, no utinam percipit lobortis usu. No gloriatur persequeris suscipiantur his, te nec modo admodum vulputate, ne pri accumsan placerat detraxit. An ius solum iudico aliquid, vim no consul platonem, amet omittantur vim eu.

<br/>

{:.highlight .grey}
Lorem ipsum dolor sit amet, assum similique delicatissimi usu no. Fierent laboramus sit ut, cetero eloquentiam eos cu. Vim falli graeco an, usu at quot graeco, sit in accumsan probatus delicata. Debet vitae iracundia per cu, no utinam percipit lobortis usu. No gloriatur persequeris suscipiantur his, te nec modo admodum vulputate, ne pri accumsan placerat detraxit. An ius solum iudico aliquid, vim no consul platonem, amet omittantur vim eu.

<br/>

{:.highlight .magenta-border}
Lorem ipsum dolor sit amet, assum similique delicatissimi usu no. Fierent laboramus sit ut, cetero eloquentiam eos cu. Vim falli graeco an, usu at quot graeco, sit in accumsan probatus delicata. Debet vitae iracundia per cu, no utinam percipit lobortis usu. No gloriatur persequeris suscipiantur his, te nec modo admodum vulputate, ne pri accumsan placerat detraxit. An ius solum iudico aliquid, vim no consul platonem, amet omittantur vim eu.

<br/><br/><br/>

##### Lsts

- item 1
- item 2
- item 3

1. item 1
2. item 2
3. item 3

 <br/><br/><br/>

##### Light and bold horizontal lines
<br/>

{:.light}
<hr/>

 <br/>

{:.bold}
<hr/>

 <br/>

{:.magenta}
<hr/>

 <br/><br/><br/>

##### Footnotes

 Lorem ipsum dolor sit amet, assum similique delicatissimi usu no. Fierent laboramus sit ut, cetero eloquentiam eos cu. Vim falli graeco an, usu at quot graeco, sit in accumsan probatus delicata. Debet vitae iracundia per cu, no utinam percipit lobortis usu. No gloriatur persequeris suscipiantur his, te nec modo admodum vulputate, ne pri accumsan placerat detraxit. An ius solum iudico aliquid, vim no consul platonem, amet omittantur vim eu.
